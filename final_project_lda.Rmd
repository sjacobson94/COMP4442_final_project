---
title: "STAT 4442 Final Project"
author: "Wayne Yandell & Sawyer Jacobson"
date: "8/9/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# Initial Exploratory Data Analysis

```{r}
library(tidyverse)
library(tidytext)
library(stm)
library(knitr)
library(stringr)
library(magrittr)
library(stringi)
library(quanteda)
library(topicmodels)
# Read in the data from the website
covid_texts <- read.csv("https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-08-08/metadata.csv") %>%   
  tibble::as_tibble()                         # Converting to tibble so it prints out cleaner


```

```{r}
# Papers that have "coronavirus" in the title
# We'll want to use publish date most likely, however the date variable is a little messy 
# so we'll grab the first 4 digits for the year to save the data

covid_2020 <- covid_texts %>%
  filter(str_detect(tolower(title), 'coronavirus|covid')) %>%
  mutate(publish_year = str_sub(publish_time, 1, 4)) %>% 
  filter(publish_year == "2020" & !str_detect(title, "Â¿") & abstract != '') %>%
  group_by(title) %>%
  slice(1) %>% # Remove duplicate titled papers (most likely the same paper published in multiple journals)
  ungroup() %>%
  filter(stri_enc_isascii(abstract)) %>%
  rowwise() %>%
  mutate(
    abstract_words = sapply(strsplit(abstract, " "), length)
  )
  

covid_2020 %>%
  write_csv("covid_2020.csv")


```

```{r}
covid_2020 <- covid_2020[c('cord_uid','abstract')] %>%
  group_by(cord_uid) %>%
  mutate(articleID = row_number()) %>%
  ungroup()

tidy_abstracts <- covid_2020 %>%
  unnest_tokens(word, abstract) %>%
  anti_join(stop_words)

tidy_abstracts %>%
  count(word, sort = TRUE) %>%
  filter(n > 8000) %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

tidy_abstracts <- tidy_abstracts %>%
  filter(!(word %in% c('covid','coronavirus', 'cov', 'patients', 'sars', 'disease','pandemic', 'health') | str_detect(word, pattern = "\\d+")))


tidy_abstracts
word_counts <- tidy_abstracts %>%
  count(cord_uid, word, sort = TRUE)

abstract_dfm <- word_counts %>%
  cast_dfm(cord_uid, word, n)

word_counts

abstract_dtm <- convert(abstract_dfm, to = "topicmodels")
```


```{r}
library(ldatuning)
result <- FindTopicsNumber(
  abstract_dtm,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)
FindTopicsNumber_plot(result)
```

```{r}
# topic_model <- stm(abstract_stm, K = 6, verbose = FALSE, init.type = "Spectral")

lda <- LDA(abstract_dtm, k = 6)
terms(lda, 10)
abstract_stm <- convert(abstract_dfm, to = "stm")
td_beta <- tidy(lda)

td_beta %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    mutate(topic = paste0("Topic ", topic),
           term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free_y") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = expression(beta),
         title = "Highest word probabilities for each topic",
         subtitle = "Different words are associated with different topics")

td_gamma <- tidy(lda, matrix = "gamma",                    
                 document_names = rownames(abstract_dfm))

ggplot(td_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 3) +
  labs(title = "Distribution of document probabilities for each topic",
       y = "Number of articles", x = expression(gamma))
```

